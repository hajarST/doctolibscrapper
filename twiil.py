from playwright.sync_api import sync_playwright
import pandas as pd
import time
import random

BASE_URL = "https://www.doctolib.fr/medecin-generaliste/?page="
START_PAGE = 1
END_PAGE = 10  # Vous pouvez mettre autant de pages que vous voulez

def automatic_captcha_bypass(page):
    """Contournement automatique des CAPTCHAs sans intervention manuelle"""
    print("üîÑ Tentative de contournement automatique du CAPTCHA...")
    
    try:
        # Strat√©gie 1: Utiliser une IP diff√©rente en rechargeant avec des param√®tres al√©atoires
        print("üéØ Strat√©gie 1: Rechargement avec param√®tres al√©atoires...")
        page.reload(wait_until='networkidle')
        time.sleep(5)
        
        # Strat√©gie 2: Nettoyer les cookies et storage
        print("üéØ Strat√©gie 2: Nettoyage des cookies...")
        page.evaluate("""
            localStorage.clear();
            sessionStorage.clear();
            document.cookie.split(";").forEach(function(c) { 
                document.cookie = c.replace(/^ +/, "").replace(/=.*/, "=;expires=" + new Date().toUTCString() + ";path=/"); 
            });
        """)
        
        # Strat√©gie 3: Changer l'user-agent dynamiquement
        print("üéØ Strat√©gie 3: Rotation de l'user-agent...")
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0'
        ]
        new_ua = random.choice(user_agents)
        page.set_extra_http_headers({'User-Agent': new_ua})
        
        # Strat√©gie 4: Naviguer vers une URL diff√©rente puis revenir
        print("üéØ Strat√©gie 4: Navigation alternative...")
        page.goto("https://www.doctolib.fr", wait_until='networkidle')
        time.sleep(3)
        page.go_back(wait_until='networkidle')
        time.sleep(5)
        
        # Strat√©gie 5: Simulation d'activit√© humaine intensive
        print("üéØ Strat√©gie 5: Simulation d'activit√© humaine...")
        simulate_intensive_human_behavior(page)
        
        # Strat√©gie 6: Attendre que le CAPTCHA disparaisse (certains sites les retirent apr√®s un certain temps)
        print("üéØ Strat√©gie 6: Attente strat√©gique...")
        time.sleep(10)
        
        # V√©rifier si le CAPTCHA est toujours pr√©sent
        captcha_selectors = [
            "text=/captcha/i",
            "text=/robot/i", 
            "text=/v√©rification/i",
            "iframe[src*='recaptcha']",
            "img[src*='captcha']",
            ".g-recaptcha"
        ]
        
        captcha_still_present = False
        for selector in captcha_selectors:
            if page.locator(selector).count() > 0:
                captcha_still_present = True
                break
        
        if not captcha_still_present:
            print("‚úÖ CAPTCHA contourn√© avec succ√®s!")
            return True
        else:
            print("‚ùå CAPTCHA toujours pr√©sent apr√®s contournement")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur lors du contournement: {e}")
        return False

def simulate_intensive_human_behavior(page):
    """Simule un comportement humain tr√®s r√©aliste et intensif"""
    print("ü§ñ Simulation comportement humain intensif...")
    
    # Mouvements de souris tr√®s r√©alistes
    viewport = page.viewport_size
    for _ in range(random.randint(8, 15)):
        x = random.randint(50, viewport['width'] - 50)
        y = random.randint(50, viewport['height'] - 50)
        
        # Mouvement fluide avec plusieurs points interm√©diaires
        steps = random.randint(3, 8)
        for step in range(steps):
            intermediate_x = x + random.randint(-20, 20)
            intermediate_y = y + random.randint(-20, 20)
            page.mouse.move(intermediate_x, intermediate_y)
            time.sleep(random.uniform(0.1, 0.3))
        
        time.sleep(random.uniform(0.5, 1.5))
    
    # Clics r√©alistes sur diff√©rents √©l√©ments
    clickable_selectors = [
        "a", "button", "div[role='button']", "input[type='submit']",
        "span", "li", ".card", ".item"
    ]
    
    for selector in random.sample(clickable_selectors, random.randint(2, 4)):
        try:
            elements = page.locator(selector)
            if elements.count() > 0:
                element = elements.nth(random.randint(0, min(3, elements.count() - 1)))
                if element.is_visible():
                    element.scroll_into_view_if_needed()
                    time.sleep(1)
                    element.click(force=True)
                    time.sleep(random.uniform(2, 4))
                    print(f"üñ±Ô∏è Clic sur √©l√©ment: {selector}")
                    break
        except:
            continue
    
    # Scroll tr√®s r√©aliste avec variations de vitesse
    scroll_patterns = [
        (300, 0.5),   # Petit scroll rapide
        (800, 1.5),   # Moyen scroll moyen
        (1500, 2.5),  # Grand scroll lent
        (500, 0.8),   # Retour rapide
    ]
    
    for scroll_amount, scroll_time in random.sample(scroll_patterns, random.randint(3, 6)):
        page.mouse.wheel(0, scroll_amount)
        time.sleep(scroll_time + random.uniform(0.5, 1.0))
    
    # Frappe au clavier simul√©e dans les champs de recherche
    try:
        search_input = page.locator("input[type='search'], input[name='search'], [role='searchbox']")
        if search_input.count() > 0:
            search_input.first.click()
            time.sleep(1)
            search_input.first.fill("")
            for char in "medecin generaliste":
                search_input.first.type(char, delay=random.randint(50, 200))
                time.sleep(random.uniform(0.1, 0.3))
            time.sleep(2)
    except:
        pass

def rotate_browser_context(context):
    """Cr√©e un nouveau contexte avec une empreinte diff√©rente"""
    print("üîÑ Rotation du contexte navigateur...")
    
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0'
    ]
    
    viewports = [
        {'width': 1920, 'height': 1080},
        {'width': 1366, 'height': 768},
        {'width': 1536, 'height': 864},
        {'width': 1440, 'height': 900}
    ]
    
    new_context = context.browser.new_context(
        viewport=random.choice(viewports),
        user_agent=random.choice(user_agents),
        extra_http_headers={
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
        }
    )
    
    # Nouveau script d'init pour masquer l'automation
    new_context.add_init_script("""
        Object.defineProperty(navigator, 'webdriver', {
            get: () => undefined,
        });
        
        Object.defineProperty(navigator, 'plugins', {
            get: () => [1, 2, 3, 4, 5],
        });
        
        Object.defineProperty(navigator, 'languages', {
            get: () => ['fr-FR', 'fr', 'en-US', 'en'],
        });
        
        // Masquer d'autres indicateurs d'automation
        window.chrome = { runtime: {} };
    """)
    
    return new_context

def scrape_doctolib():
    data = []
    failed_pages = 0
    max_failed_pages = 3  # Maximum de pages √©chou√©es avant arr√™t

    with sync_playwright() as p:
        # Configuration avanc√©e pour √©viter la d√©tection
        browser = p.chromium.launch(
            headless=False,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-features=VizDisplayCompositor',
                '--no-first-run',
                '--disable-default-apps',
                '--disable-extensions',
                '--disable-plugins',
                '--disable-translate',
                '--disable-dev-shm-usage',
                '--no-sandbox',
            ]
        )
        
        # Contexte initial
        context = browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        )

        # Script pour masquer l'automation
        context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined,
            });
        """)

        for page_number in range(START_PAGE, END_PAGE + 1):
            if failed_pages >= max_failed_pages:
                print(f"üö® Trop de pages √©chou√©es ({failed_pages}). Arr√™t du scraping.")
                break
                
            url = BASE_URL + str(page_number)
            print(f"\nüîç Chargement de la page {page_number} : {url}")
            
            page = context.new_page()
            
            try:
                # Navigation avec timeout long
                page.goto(url, timeout=120000, wait_until='networkidle')
                
                # Simulation comportement humain avant v√©rification
                simulate_intensive_human_behavior(page)
                
                # V√©rifier si un CAPTCHA est pr√©sent
                captcha_selectors = [
                    "text=/captcha/i",
                    "text=/robot/i", 
                    "text=/v√©rification/i",
                    "iframe[src*='recaptcha']",
                    "img[src*='captcha']",
                    ".g-recaptcha"
                ]
                
                captcha_detected = False
                for selector in captcha_selectors:
                    if page.locator(selector).count() > 0:
                        captcha_detected = True
                        print(f"üõë CAPTCHA d√©tect√© avec le s√©lecteur: {selector}")
                        break
                
                if captcha_detected:
                    print("üîÑ Tentative de contournement automatique du CAPTCHA...")
                    if automatic_captcha_bypass(page):
                        print("‚úÖ CAPTCHA contourn√© avec succ√®s!")
                        # Continuer le scraping normalement
                    else:
                        print("‚ùå √âchec du contournement, rotation du contexte...")
                        page.close()
                        
                        # Rotation du contexte navigateur
                        context.close()
                        context = rotate_browser_context(context)
                        failed_pages += 1
                        continue
                
                # Scroll r√©aliste pour charger tous les r√©sultats
                scroll_steps = random.randint(5, 8)
                for i in range(scroll_steps):
                    scroll_amount = random.randint(800, 1500)
                    page.mouse.wheel(0, scroll_amount)
                    time.sleep(random.uniform(2, 4))
                    human_like_behavior(page)
                
                # Attendre que les r√©sultats se chargent
                page.wait_for_timeout(5000)

                # R√©cup√©rer TOUS les liens de profils
                try:
                    page.wait_for_selector("div.flex.justify-between > a[href*='/medecin-generaliste/']", timeout=15000)
                    links = page.eval_on_selector_all(
                        "div.flex.justify-between > a[href*='/medecin-generaliste/']",
                        "elements => elements.map(el => el.href)"
                    )
                    links = list(set(links))
                    print(f"üîó {len(links)} profils trouv√©s sur la page {page_number}.")
                except Exception as e:
                    print(f"‚ùå Aucune donn√©e trouv√©e sur cette page: {e}")
                    page.close()
                    failed_pages += 1
                    continue

                # R√©initialiser le compteur d'√©checs en cas de succ√®s
                failed_pages = 0

                # Scraper TOUS les profils sans limitation
                for idx, link in enumerate(links):
                    try:
                        print(f"üìñ Traitement du profil {idx+1}/{len(links)}")
                        
                        # Pause al√©atoire entre les profils
                        time.sleep(random.uniform(3, 7))
                        
                        profile_page = context.new_page()
                        
                        # Comportement humain avant d'aller sur le profil
                        human_like_behavior(profile_page)
                        
                        profile_page.goto(link, timeout=60000, wait_until='domcontentloaded')
                        
                        # V√©rifier CAPTCHA sur la page de profil
                        captcha_detected_profile = False
                        for selector in captcha_selectors:
                            if profile_page.locator(selector).count() > 0:
                                captcha_detected_profile = True
                                break
                        
                        if captcha_detected_profile:
                            print("üõë CAPTCHA d√©tect√© sur le profil, tentative de contournement...")
                            if automatic_captcha_bypass(profile_page):
                                print("‚úÖ CAPTCHA contourn√© sur le profil!")
                            else:
                                print("‚ùå √âchec du contournement sur le profil, passage au suivant...")
                                profile_page.close()
                                continue
                        
                        profile_page.wait_for_timeout(3000)

                        # Extraction des donn√©es
                        # Nom
                        try:
                            profile_page.wait_for_selector("h1#profile-name-with-title span[itemprop='name']", timeout=5000)
                            name = profile_page.locator("h1#profile-name-with-title span[itemprop='name']").inner_text().strip()
                        except:
                            name = "Nom non trouv√©"

                        # Fonction
                        try:
                            fonction = profile_page.locator("div.dl-profile-header-speciality span").inner_text().strip()
                        except:
                            fonction = "Fonction non trouv√©e"

                        # T√©l√©phone
                        try:
                            phone = profile_page.locator("div.dl-profile-box:has(h3:has-text('Coordonn√©es')) div.flex").inner_text().strip()
                        except:
                            phone = "T√©l√©phone non trouv√©"

                        # Exp√©rience
                        experience_list = []
                        try:
                            exp_container = profile_page.locator("h3.dl-profile-card-title:has-text('Exp√©rience')").locator("..")
                            entries = exp_container.locator("div.dl-profile-text.dl-profile-entry")
                            for i in range(entries.count()):
                                time_text = entries.nth(i).locator("div.dl-profile-entry-time").inner_text().strip()
                                label_text = entries.nth(i).locator("p.dl-profile-entry-label").inner_text().strip()
                                if time_text:
                                    experience_list.append(f"{label_text} ({time_text})")
                                else:
                                    experience_list.append(label_text)
                            experience = "\n".join(experience_list)
                        except:
                            experience = "Exp√©rience non pr√©cis√©e"

                        # Dipl√¥mes
                        dipl√¥mes_list = []
                        try:
                            dipl_container = profile_page.locator("h3.dl-profile-card-title:has-text('Dipl√¥mes nationaux et universitaires')").locator("..")
                            dipl_entries = dipl_container.locator("div.dl-profile-text.dl-profile-entry")
                            for i in range(dipl_entries.count()):
                                year = dipl_entries.nth(i).locator("div.dl-profile-entry-time").inner_text().strip()
                                label = dipl_entries.nth(i).locator("p.dl-profile-entry-label").inner_text().strip()
                                dipl√¥mes_list.append(f"{label} ({year})")
                            diplomes = "\n".join(dipl√¥mes_list)
                        except:
                            diplomes = "Non pr√©cis√©s"

                        # Adresse
                        try:
                            cabinet_name = profile_page.locator("div.dl-profile-text div .dl-profile-practice-name").inner_text().strip()
                        except:
                            cabinet_name = ""

                        try:
                            address_element = profile_page.locator("div.dl-profile-text div div[id^='practice-address-']")
                            address_text = address_element.inner_text().strip()
                        except:
                            address_text = ""

                        if cabinet_name or address_text:
                            address = f"{cabinet_name} - {address_text}".strip(" -")
                        else:
                            address = "Adresse non trouv√©e"

                        # Horaires
                        try:
                            horaires_list = []
                            horaires_elements = profile_page.locator("div.js-opening-hours ul li div[itemprop='openingHours']")
                            for i in range(horaires_elements.count()):
                                horaires_list.append(horaires_elements.nth(i).inner_text().strip())
                            horaires = "\n".join(horaires_list)
                        except:
                            horaires = "Horaires non trouv√©s"

                        # Num√©ro RPPS
                        try:
                            rpps = profile_page.locator("p:has-text('Num√©ro RPPS') + p").inner_text().strip()
                        except:
                            rpps = "Non trouv√©"

                        # SIREN
                        try:
                            siren = profile_page.locator("p:has-text('SIREN') + p").inner_text().strip()
                        except:
                            siren = "Non trouv√©"

                        # Ajouter au tableau
                        data.append({
                            "Nom": name,
                            "Fonction": fonction,
                            "T√©l√©phone": phone,
                            "Exp√©rience": experience,
                            "Dipl√¥mes": diplomes,
                            "Adresse": address,
                            "Horaires": horaires,
                            "Num√©ro RPPS": rpps,
                            "SIREN": siren,
                            "Lien": link
                        })

                        print(f"‚úÖ Profil {len(data)} extrait : {name}")
                        profile_page.close()

                    except Exception as e:
                        print(f"‚ö†Ô∏è Erreur lors de l'extraction du profil {link}: {e}")
                        try:
                            profile_page.close()
                        except:
                            pass
                        continue

                page.close()
                
                # Pause strat√©gique entre les pages
                if page_number < END_PAGE:
                    pause_time = random.randint(15, 30)
                    print(f"‚è≥ Pause de {pause_time} secondes avant la page suivante...")
                    time.sleep(pause_time)

            except Exception as e:
                print(f"‚ùå Erreur sur la page {page_number}: {e}")
                try:
                    page.close()
                except:
                    pass
                failed_pages += 1
                continue

        browser.close()

    # Sauvegarde Excel
    if data:
        df = pd.DataFrame(data)
        filename = f"doctolib_doctors_auto_captcha_{START_PAGE}_to_{END_PAGE}.xlsx"
        df.to_excel(filename, index=False, engine='openpyxl')
        print(f"\nüìÅ Donn√©es enregistr√©es dans {filename} (Total : {len(data)} profils) ‚úÖ")
    else:
        print("‚ùå Aucune donn√©e collect√©e")

def human_like_behavior(page):
    """Simule un comportement humain r√©aliste"""
    # Mouvements de souris al√©atoires
    for _ in range(random.randint(2, 4)):
        x = random.randint(100, page.viewport_size['width'] - 100)
        y = random.randint(100, page.viewport_size['height'] - 100)
        page.mouse.move(x, y)
        time.sleep(random.uniform(0.5, 1.5))
    
    # Clics al√©atoires occasionnels
    if random.random() > 0.8:
        page.mouse.click(x, y)
        time.sleep(1)

if __name__ == "__main__":
    print("üöÄ D√©marrage du scraping Doctolib avec contournement automatique des CAPTCHAs...")
    print("üéØ Strat√©gie: Rotation d'empreinte navigateur + Comportement humain intensif")
    print("‚ö° Aucune limitation - Tous les profils seront scrap√©s")
    scrape_doctolib()